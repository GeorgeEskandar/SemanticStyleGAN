{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisson Notebook\n",
    "\n",
    " This notebook is used to generate comparisson plots between our implemntation, SP's implementation, and stylegan2 to show the differences in controllability between the three approaches and show each approach's capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "CUDA_HOME environment variable is not set. Please set it to your CUDA install root.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/usrhomes/g013/SemanticStyleGAN/notebooks/generate_comparison_plots.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blogin1/usrhomes/g013/SemanticStyleGAN/notebooks/generate_comparison_plots.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1/usrhomes/g013/SemanticStyleGAN/notebooks/generate_comparison_plots.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m../../SemanticStyleGAN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blogin1/usrhomes/g013/SemanticStyleGAN/notebooks/generate_comparison_plots.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m make_model\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1/usrhomes/g013/SemanticStyleGAN/notebooks/generate_comparison_plots.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mvisualize\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m generate, cubic_spline_interpolate\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blogin1/usrhomes/g013/SemanticStyleGAN/notebooks/generate_comparison_plots.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrol\u001b[39;00m \u001b[39mimport\u001b[39;00m Control\n",
      "File \u001b[0;32m/misc/usrhomes/g013/SemanticStyleGAN/notebooks/../../SemanticStyleGAN/models/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (C) 2022 ByteDance Inc.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpprint\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msemantic_stylegan\u001b[39;00m \u001b[39mimport\u001b[39;00m SemanticGenerator, DualBranchDiscriminator\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_model\u001b[39m(args, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/misc/usrhomes/g013/SemanticStyleGAN/notebooks/../../SemanticStyleGAN/models/semantic_stylegan.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m     StyledConv,\n\u001b[1;32m     28\u001b[0m     FixedStyledConv,\n\u001b[1;32m     29\u001b[0m     ToRGB,\n\u001b[1;32m     30\u001b[0m     PixelNorm,\n\u001b[1;32m     31\u001b[0m     EqualLinear,\n\u001b[1;32m     32\u001b[0m     ConvLayer,\n\u001b[1;32m     33\u001b[0m     ResBlock,\n\u001b[1;32m     34\u001b[0m     PositionEmbedding,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLocalGenerator\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     39\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     40\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     41\u001b[0m         in_channel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m         detach_texture\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     49\u001b[0m     ):\n",
      "File \u001b[0;32m/misc/usrhomes/g013/SemanticStyleGAN/notebooks/../../SemanticStyleGAN/models/utils.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparametrizations\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mparametrizations\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mop\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedLeakyReLU, fused_leaky_relu, upfirdn2d, conv2d_gradfix\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBlur\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     20\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, kernel, pad, upsample_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m/misc/usrhomes/g013/SemanticStyleGAN/notebooks/../../SemanticStyleGAN/models/op/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfused_act\u001b[39;00m \u001b[39mimport\u001b[39;00m FusedLeakyReLU, fused_leaky_relu\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mupfirdn2d\u001b[39;00m \u001b[39mimport\u001b[39;00m upfirdn2d\n",
      "File \u001b[0;32m/misc/usrhomes/g013/SemanticStyleGAN/notebooks/../../SemanticStyleGAN/models/op/fused_act.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcpp_extension\u001b[39;00m \u001b[39mimport\u001b[39;00m load\n\u001b[1;32m     17\u001b[0m module_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m fused \u001b[39m=\u001b[39m load(\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m     sources\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     21\u001b[0m         os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(module_path, \u001b[39m\"\u001b[39;49m\u001b[39mfused_bias_act.cpp\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     22\u001b[0m         os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(module_path, \u001b[39m\"\u001b[39;49m\u001b[39mfused_bias_act_kernel.cu\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     23\u001b[0m     ],\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFusedLeakyReLUFunctionBackward\u001b[39;00m(Function):\n\u001b[1;32m     28\u001b[0m     \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     29\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, grad_output, out, bias, negative_slope, scale):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1202\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name,\n\u001b[1;32m   1112\u001b[0m          sources: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m   1113\u001b[0m          extra_cflags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m          is_standalone\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1122\u001b[0m          keep_intermediates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[39m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[39m                verbose=True)\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m-> 1202\u001b[0m     \u001b[39mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1203\u001b[0m         name,\n\u001b[1;32m   1204\u001b[0m         [sources] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(sources, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m sources,\n\u001b[1;32m   1205\u001b[0m         extra_cflags,\n\u001b[1;32m   1206\u001b[0m         extra_cuda_cflags,\n\u001b[1;32m   1207\u001b[0m         extra_ldflags,\n\u001b[1;32m   1208\u001b[0m         extra_include_paths,\n\u001b[1;32m   1209\u001b[0m         build_directory \u001b[39mor\u001b[39;49;00m _get_build_directory(name, verbose),\n\u001b[1;32m   1210\u001b[0m         verbose,\n\u001b[1;32m   1211\u001b[0m         with_cuda,\n\u001b[1;32m   1212\u001b[0m         is_python_module,\n\u001b[1;32m   1213\u001b[0m         is_standalone,\n\u001b[1;32m   1214\u001b[0m         keep_intermediates\u001b[39m=\u001b[39;49mkeep_intermediates)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1425\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 hipified_sources\u001b[39m.\u001b[39madd(hipify_result[s_abs][\u001b[39m\"\u001b[39m\u001b[39mhipified_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m s_abs \u001b[39min\u001b[39;00m hipify_result \u001b[39melse\u001b[39;00m s_abs)\n\u001b[1;32m   1423\u001b[0m             sources \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(hipified_sources)\n\u001b[0;32m-> 1425\u001b[0m         _write_ninja_file_and_build_library(\n\u001b[1;32m   1426\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1427\u001b[0m             sources\u001b[39m=\u001b[39;49msources,\n\u001b[1;32m   1428\u001b[0m             extra_cflags\u001b[39m=\u001b[39;49mextra_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1429\u001b[0m             extra_cuda_cflags\u001b[39m=\u001b[39;49mextra_cuda_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1430\u001b[0m             extra_ldflags\u001b[39m=\u001b[39;49mextra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1431\u001b[0m             extra_include_paths\u001b[39m=\u001b[39;49mextra_include_paths \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1432\u001b[0m             build_directory\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   1433\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1434\u001b[0m             with_cuda\u001b[39m=\u001b[39;49mwith_cuda,\n\u001b[1;32m   1435\u001b[0m             is_standalone\u001b[39m=\u001b[39;49mis_standalone)\n\u001b[1;32m   1436\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1437\u001b[0m     baton\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1514\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39mif\u001b[39;00m with_cuda \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1513\u001b[0m     with_cuda \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\u001b[39mmap\u001b[39m(_is_cuda_file, sources))\n\u001b[0;32m-> 1514\u001b[0m extra_ldflags \u001b[39m=\u001b[39m _prepare_ldflags(\n\u001b[1;32m   1515\u001b[0m     extra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1516\u001b[0m     with_cuda,\n\u001b[1;32m   1517\u001b[0m     verbose,\n\u001b[1;32m   1518\u001b[0m     is_standalone)\n\u001b[1;32m   1519\u001b[0m build_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(build_directory, \u001b[39m'\u001b[39m\u001b[39mbuild.ninja\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1520\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1622\u001b[0m, in \u001b[0;36m_prepare_ldflags\u001b[0;34m(extra_ldflags, with_cuda, verbose, is_standalone)\u001b[0m\n\u001b[1;32m   1620\u001b[0m         extra_ldflags\u001b[39m.\u001b[39mappend(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(CUDNN_HOME, \u001b[39m'\u001b[39m\u001b[39mlib/x64\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1621\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m IS_HIP_EXTENSION:\n\u001b[0;32m-> 1622\u001b[0m     extra_ldflags\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-L\u001b[39m\u001b[39m{\u001b[39;00m_join_cuda_home(\u001b[39m\"\u001b[39m\u001b[39mlib64\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1623\u001b[0m     extra_ldflags\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m-lcudart\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1624\u001b[0m     \u001b[39mif\u001b[39;00m CUDNN_HOME \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/cpp_extension.py:2125\u001b[0m, in \u001b[0;36m_join_cuda_home\u001b[0;34m(*paths)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[39mJoins paths with CUDA_HOME, or raises an error if it CUDA_HOME is not set.\u001b[39;00m\n\u001b[1;32m   2120\u001b[0m \n\u001b[1;32m   2121\u001b[0m \u001b[39mThis is basically a lazy way of raising an error for missing $CUDA_HOME\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[39monly once we need to get any CUDA-specific path.\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m \u001b[39mif\u001b[39;00m CUDA_HOME \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCUDA_HOME environment variable is not set. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2126\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mPlease set it to your CUDA install root.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   2127\u001b[0m \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(CUDA_HOME, \u001b[39m*\u001b[39mpaths)\n",
      "\u001b[0;31mOSError\u001b[0m: CUDA_HOME environment variable is not set. Please set it to your CUDA install root."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "import imageio\n",
    "from itertools import chain\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, \"../../SemanticStyleGAN\")\n",
    "from models import make_model\n",
    "from visualize.utils import generate, cubic_spline_interpolate\n",
    "from utils.control import Control\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "sys.path.insert(0,\"/no_backups/g013/other_GANs/stylegan2-pytorch\")\n",
    "from model import Generator as StyleGANGenerator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate images using Semantic Palette\n",
    "Manipulation itself is done on the side of semantic palette's code of visualization ( interpolation of semantic condition) here we mainly only change the condition $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem0_18 = torch.load(f\"./data_for_comparison/sem_0_18.pt\")\n",
    "sem1_18 = torch.load(f\"./data_for_comparison/sem_1_18.pt\")\n",
    "sem0_20 = torch.load(f\"./data_for_comparison/sem_0_20.pt\")\n",
    "sem1_20 = torch.load(f\"./data_for_comparison/sem_1_20.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of road in sem0 :39.28250503540039  vegetation 11.405961990356445 car :3.828788995742798\n",
      "distribution of road in new_sem :24.282506942749023  vegetation 6.405961513519287 car :23.82878875732422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating a new sem1 by modifying sem0 which has a high dist of road and vegitation to have \n",
    "#instead a higher dist of cars\n",
    "\n",
    "#Editing Image 18\n",
    "car_index=26\n",
    "vegi_index=21\n",
    "road_index=7\n",
    "new_sem = sem0_18.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{sem0_18[0][road_index]*100}  vegetation {sem0_18[0][vegi_index]*100} car :{sem0_18[0][car_index]*100}\")\n",
    "new_sem[0][road_index]-=0.15\n",
    "new_sem[0][vegi_index]-=0.05\n",
    "new_sem[0][car_index]+=0.20\n",
    "print(f\"distribution of road in new_sem :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "\n",
    "torch.save(new_sem,f\"./data_for_comparison/sem_1_18_edited.pt\")\n",
    "new_sem.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of road in sem0 :32.07407760620117  vegetation 6.262269020080566 car :21.392187118530273\n",
      "distribution of road in new_sem :17.074077606201172  vegetation 1.2622687816619873 car :41.392189025878906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Editing Image 20\n",
    "car_index=26\n",
    "vegi_index=21\n",
    "road_index=7\n",
    "new_sem = sem0_20.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{sem0_20[0][road_index]*100}  vegetation {sem0_20[0][vegi_index]*100} car :{sem0_20[0][car_index]*100}\")\n",
    "new_sem[0][road_index]-=0.15\n",
    "new_sem[0][vegi_index]-=0.05\n",
    "new_sem[0][car_index]+=0.20\n",
    "print(f\"distribution of road in new_sem :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "\n",
    "torch.save(new_sem,f\"./data_for_comparison/sem_1_20_edited.pt\")\n",
    "new_sem.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0149)\n",
      "tensor(0.3883)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "print(sem1_18[0][26])\n",
    "print(sem1_18[0][7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another comparisson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem0_4 = torch.load(f\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_2/picked_image/sem_0_4.pt\")\n",
    "sem1_4 = torch.load(f\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_2/picked_image/sem_1_4.pt\")\n",
    "sem0_2 = torch.load(f\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2.pt\")\n",
    "sem1_2 = torch.load(f\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_1_2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of road in sem0 :35.59489822387695  vegetation 8.172882080078125 car :10.403952598571777\n",
      "tensor(1.)\n",
      "distribution of road in sem0 :48.59489822387695  vegetation 0.17288252711296082 car :5.403952598571777\n",
      "distribution of road in sem0 :35.59489822387695  vegetation 8.172882080078125 car :10.403952598571777\n",
      "tensor(1.0000)\n",
      "distribution of road in sem0 :15.594898223876953  vegetation 23.172882080078125 car :15.403953552246094\n",
      "distribution of road in sem0 :35.59489822387695  vegetation 8.172882080078125 car :10.403952598571777\n",
      "tensor(1.)\n",
      "distribution of road in sem0 :20.594898223876953  vegetation 3.172882318496704 car :30.403953552246094\n",
      "distribution of road in sem0 :35.59489822387695  vegetation 8.172882080078125 car :10.403952598571777\n",
      "tensor(1.0000)\n",
      "distribution of road in sem0 :40.09490203857422  vegetation 12.672883033752441 car :1.4039523601531982\n",
      "distribution of road in sem0 :35.59489822387695  vegetation 8.172882080078125 car :10.403952598571777\n",
      "tensor(1.)\n",
      "distribution of road in sem0 :18.594898223876953  vegetation 28.172883987426758 car :7.403952598571777\n",
      "distribution of road in sem0 :35.59489822387695  vegetation 8.172882080078125 car :10.403952598571777\n",
      "tensor(1.0720)\n",
      "distribution of road in sem0 :45.59489822387695  vegetation 0.3728821873664856 car :15.403953552246094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0720)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating a new sem1 by modifying sem0 which has a high dist of road and vegitation to have \n",
    "#instead a higher dist of cars\n",
    "\n",
    "#Editing Image 4\n",
    "car_index=26\n",
    "vegi_index=21\n",
    "road_index=7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Increase Road\n",
    "new_sem = sem1_2.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "new_sem[0][road_index]+=0.13\n",
    "new_sem[0][vegi_index]-=0.08\n",
    "new_sem[0][car_index]-=0.05\n",
    "print(new_sem.sum())\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "torch.save(new_sem,\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2_more_road.pt\")\n",
    "\n",
    "#Decrease Road\n",
    "new_sem = sem1_2.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "new_sem[0][road_index]-=0.20\n",
    "new_sem[0][vegi_index]+=0.15\n",
    "new_sem[0][car_index]+=0.05\n",
    "print(new_sem.sum())\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "torch.save(new_sem,\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2_less_road.pt\")\n",
    "\n",
    "#Increase Car\n",
    "new_sem = sem1_2.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "new_sem[0][road_index]-=0.15\n",
    "new_sem[0][vegi_index]-=0.05\n",
    "new_sem[0][car_index]+=0.20\n",
    "print(new_sem.sum())\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "torch.save(new_sem,\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2_more_car.pt\")\n",
    "\n",
    "#Decrease Car\n",
    "\n",
    "new_sem = sem1_2.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "new_sem[0][road_index]+=0.045\n",
    "new_sem[0][vegi_index]+=0.045\n",
    "new_sem[0][car_index]-=0.09\n",
    "print(new_sem.sum())\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "torch.save(new_sem,\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2_less_car.pt\")\n",
    "\n",
    "\n",
    "#Increase vegi\n",
    "new_sem = sem1_2.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "new_sem[0][road_index]-=0.17\n",
    "new_sem[0][vegi_index]+=0.20\n",
    "new_sem[0][car_index]-=0.03\n",
    "print(new_sem.sum())\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "torch.save(new_sem,\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2_more_vegi.pt\")\n",
    "\n",
    "#Decrease Vegi\n",
    "new_sem = sem1_2.detach().clone()\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "new_sem[0][road_index]+=0.10\n",
    "new_sem[0][vegi_index]-=0.078\n",
    "new_sem[0][car_index]+=0.05\n",
    "print(new_sem.sum())\n",
    "print(f\"distribution of road in sem0 :{new_sem[0][road_index]*100}  vegetation {new_sem[0][vegi_index]*100} car :{new_sem[0][car_index]*100}\")\n",
    "torch.save(new_sem,\"/no_backups/g013/other_GANs/SemanticPalette/our_analysis/version_3/picked_samples/sem_0_2_less_vegi.pt\")\n",
    "\n",
    "#torch.save(new_sem,f\"./data_for_comparison/sem_1_18_edited.pt\")\n",
    "new_sem.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate images using SSG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Editing the 2 images using 2 directions basicaly. one for increasing all of the side cars in the image\n",
    "and the other is for increasing the cars on the left for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Initializing model with arguments:\n",
      "{'aug': False,\n",
      " 'base_layers': 2,\n",
      " 'batch': 4,\n",
      " 'channel_multiplier': 2,\n",
      " 'checkpoint_dir': '/no_backups/g013/checkpoints/SSG_v3.13',\n",
      " 'ckpt': None,\n",
      " 'coarse_channel': 512,\n",
      " 'coarse_size': 64,\n",
      " 'd_reg_every': 16,\n",
      " 'dataset': '/no_backups/g013/data/lmdb_datasets/lmdb_v3.3',\n",
      " 'depth_layers': 6,\n",
      " 'detach_texture': False,\n",
      " 'distributed': True,\n",
      " 'g_reg_every': 4,\n",
      " 'inception': '/no_backups/g013/data/inception_models/inception_v3.3.pkl',\n",
      " 'iter': 600001,\n",
      " 'lambda_mask': 100.0,\n",
      " 'latent': 512,\n",
      " 'local_channel': 64,\n",
      " 'local_layers': 10,\n",
      " 'local_rank': 0,\n",
      " 'lr': 0.002,\n",
      " 'min_feat_size': 16,\n",
      " 'mixing': 0.3,\n",
      " 'n_gpu': 4,\n",
      " 'n_mlp': 8,\n",
      " 'n_sample': 16,\n",
      " 'num_workers': 8,\n",
      " 'path_batch_shrink': 2,\n",
      " 'path_regularize': 0.5,\n",
      " 'r1_img': 10,\n",
      " 'r1_seg': 1000,\n",
      " 'residual_refine': True,\n",
      " 'save_every': 5000,\n",
      " 'seg_dim': 16,\n",
      " 'size': 256,\n",
      " 'start_iter': 0,\n",
      " 'transparent_dims': (10, 12),\n",
      " 'viz_every': 2000}\n"
     ]
    }
   ],
   "source": [
    "ckpt= \"/no_backups/g013/checkpoints/SSG_v3.13/ckpt/140000.pt\"\n",
    "device=\"cpu\"\n",
    "control = Control(ckpt,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "latent_img_18= \"./data/comparison_data/img_0_0_18.npy\"\n",
    "latent_img_20= \"./data/comparison_data/img_0_0_20.npy\"\n",
    "\n",
    "styles_18 = torch.tensor(np.load(latent_img_18), device=device)\n",
    "styles_20 = torch.tensor(np.load(latent_img_20), device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing component 3\n",
      "Styles1\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "## Generating sequence for 2 images Using PCA direction in W Space \n",
    "V = torch.load(\"data/principal_components/principal_components_2.pt\")\n",
    "\n",
    "## Effects of continous changing of 1 component\n",
    "apply_to_all_layers=False\n",
    "latent_index=28\n",
    "class_index=13 #Car\n",
    "latent_name=\"car_shape\"\n",
    "component=9\n",
    "\n",
    "# chain_1 = range(0,0,1) # Set aside for now \n",
    "# chain_2 =  np.arange(10,-10,-0.1)\n",
    "# mult_range = chain(chain_1,chain_2)\n",
    "\n",
    "for component in [3]:\n",
    "    print(f\"Processing component {component}\")\n",
    "    for i,style_chosen in enumerate([styles_18]):\n",
    "        print(f\"Styles{i+1}\")       \n",
    "        images=[]\n",
    "        segs=[]\n",
    "        #for multiplier in  np.linspace(30,-40,70):\n",
    "        for multiplier in  np.linspace(50,-60,70):\n",
    "            #print(f\"Analyzing COMPONENT {component} with multiplier {multiplier}\")\n",
    "            image,seg=control.edit_image_principal_component(latent_index,class_index,multiplier,style_chosen,V[component],whole_image=apply_to_all_layers,plot=False,get_image=True)\n",
    "            images.append(image[0])\n",
    "            segs.append(seg[0])\n",
    "        # images = images + images[::-1]\n",
    "        # segs= segs + segs[::-1]\n",
    "        control.images_to_video(images,segs,f\"./data/comparison_data/output/editing_principal_component_{component}_for_car{i+1}.mp4\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Editing using Sefa based directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_w_to_w_extended(model,w_vectors,local_generator,layer):\n",
    "    '''\n",
    "    A function that converts given w_vectors of size Nx512 to Nx64 given a specific demodulation in the model.\n",
    "    '''\n",
    "    modulation=control.model.__getattr__(\"local_nets\")[local_generator].__getattr__(\"linears\")[layer].__getattr__(\"conv\").modulation\n",
    "    return modulation(w_vectors)\n",
    "    \n",
    "def prepare_w_extended(model,style,w_extended,class_index,layers):\n",
    "    assert len(style.shape)==1\n",
    "    for layer_index in layers:\n",
    "         w_extended[class_index][layer_index]=convert_from_w_to_w_extended(model,style,class_index,layer_index).detach()\n",
    "    return w_extended\n",
    "\n",
    "def calculate_sefa(model,local_nets,layers):\n",
    "    '''\n",
    "    Function to get certain weights out of the SSG Model.\n",
    "    local_nets between 0 and 15\n",
    "    layers between 0 and 9\n",
    "    '''\n",
    "    all_local_nets=model.__getattr__(\"local_nets\")\n",
    "    weights = []\n",
    "    if not type(layers)==list:\n",
    "        layers=[layers]\n",
    "    for l_net in local_nets:\n",
    "        for layer in layers:\n",
    "            weight_temp=all_local_nets[l_net].__getattr__(\"linears\")[layer].__getattr__(\"conv\").weight.squeeze(0)\n",
    "            weight_temp = weight_temp.flip(2, 3).permute(1, 0, 2, 3).flatten(1)\n",
    "            weights.append(weight_temp.cpu().detach().numpy())\n",
    "    weight = np.concatenate(weights, axis=1).astype(np.float32)\n",
    "    weight = weight / np.linalg.norm(weight, axis=0, keepdims=True)\n",
    "    eigen_values, eigen_vectors = np.linalg.eig(weight.dot(weight.T))\n",
    "\n",
    "    return eigen_vectors.T,eigen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class_index : 13  layer index: [3] with components [1]\n",
      "Processing component 1\n",
      "Styles1\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "Styles2\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "Processing class_index : 13  layer index: [4] with components [0, 3]\n",
      "Processing component 0\n",
      "Styles1\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "Styles2\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "Processing component 3\n",
      "Styles1\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "Styles2\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n",
      "in\n"
     ]
    }
   ],
   "source": [
    "## Effects of continous changing of 1 component\n",
    "\n",
    "apply_to_all_layers=False\n",
    "latent_index=4\n",
    "\n",
    "\n",
    "component=9\n",
    "coarse_layer=1\n",
    "w_extended=torch.zeros(16,10,64)\n",
    "shape_layers=[2,3,4,5]\n",
    "texture_layers=[6,7,8,9]\n",
    "#Training list of format :\n",
    "'''\n",
    "[ [Class_index,LocalGeneratorLayer, [V_components]] , ...another training param..]\n",
    "'''\n",
    "\n",
    "                \n",
    "training_list = [[13,[3],[1]],\n",
    "                [13,[4],[0,3]],]\n",
    "\n",
    "styles_list= [styles_20,styles_18]\n",
    "saved_pcas=[]\n",
    "#V=calculate_pca_from_s_space(control.model,class_index,layer_index)\n",
    "for training_instance in training_list:\n",
    "    class_index = training_instance[0]\n",
    "    layer_index= training_instance[1]\n",
    "    components = training_instance[2]\n",
    "    print(f\"Processing class_index : {class_index}  layer index: {layer_index} with components {components}\")\n",
    "    V,_=calculate_sefa(control.model,[class_index],layer_index)\n",
    "    #V=calculate_pca_from_s_space(control.model,class_index,layer_index[0])\n",
    "    #saved_pcas.append(V)\n",
    "    #np.save(f\"./thesis_related_results/saved_numpy/layer_{layer_index[0]}_class_index_{class_index}.npy\",V)\n",
    "    #V = np.load(f\"data/pca_new_analysis/pca_for_layer_{}.npy\")\n",
    "    for component in components:\n",
    "        print(f\"Processing component {component}\")\n",
    "        for i,style_chosen in enumerate(styles_list):\n",
    "            print(f\"Styles{i+1}\")    \n",
    "            images=[]\n",
    "            segs=[]\n",
    "            for multiplier in  np.linspace(-60,60,45):\n",
    "                w_extended_copy = w_extended.clone().detach()\n",
    "                w_extended_copy=prepare_w_extended(control.model,style_chosen[0][0],w_extended_copy,class_index,layer_index)\n",
    "                w_extended_copy[class_index][layer_index[0]]+=(multiplier*V[component])\n",
    "                #print(f\"Analyzing COMPONENT {component} with multiplier {multiplier}\")\n",
    "                image,seg= control.edit_image_inject_modulation(class_index,style_chosen,w_extended_copy,plot=False,get_image=True)\n",
    "                images.append(image[0])\n",
    "                segs.append(seg[0])\n",
    "\n",
    "            control.images_to_video(images,segs,f\"./data/comparison_data/output/SEFA_S_{component}_component_for_image_{i+1}__class_{class_index}_layer_{layer_index}.mp4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StyleGAN2 Comparisson"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First we will explore some of StyleGAN2's latent space using GANSPACE, Then we will invert an image that we will explore both on the side of stylegan2 and SSG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring StyleGAN2 LatentSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pca_2(samples,selected=10):\n",
    "    samples_cop = samples.cpu().detach().numpy()\n",
    "    M = mean(samples_cop)\n",
    "    C = samples_cop-M\n",
    "    V_2=cov(C.T)\n",
    "    values, vectors = eig(V_2)\n",
    "    return values[:selected],vectors[:selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load StyleGAN2 Model\n",
    "\n",
    "ckpt=\"/no_backups/g013/checkpoints/StyleGAN2_CityScapes/250000__16.pt\"\n",
    "latent = 512\n",
    "n_mlp = 8\n",
    "size=256\n",
    "device=\"cpu\"\n",
    "g_ema = StyleGANGenerator(\n",
    "    size,latent, n_mlp, channel_multiplier=2\n",
    ").to(device)\n",
    "checkpoint = torch.load(ckpt,map_location=torch.device('cpu'))\n",
    "g_ema.load_state_dict(checkpoint[\"g_ema\"],strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample N style vectors to calculate PCA\n",
    "sample_z = torch.randn(50000, latent, device=device)\n",
    "sample_w = g_ema.get_latent(sample_z)\n",
    "\n",
    "\n",
    "#Calculate PCA and retrieve eigenvectors.\n",
    "# res = calculate_pca_2(sample_w,selected=513)\n",
    "# vectors = res[1]\n",
    "# vectors = torch.tensor(vectors)\n",
    "# torch.save(vectors,\"data/comparison_data/principal_components_style_gan.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explore StyleGAN2 directions:\n",
    "# st1 = torch.load(\"data/stylegan2_samples/bielefeld_000000_005068_leftImg8bit.pt\",map_location=torch.device('cpu')) \n",
    "# st2 = torch.load(\"data/stylegan2_samples/bielefeld_000000_033979_leftImg8bit.pt\",map_location=torch.device('cpu')) \n",
    "# style_1=st1[list(st1.keys())[0]][\"latent\"].unsqueeze(0)\n",
    "# style_2=st2[list(st2.keys())[0]][\"latent\"]\n",
    "\n",
    "# print(style_1.shape)\n",
    "# print(style_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_to_video(images, save_dir,fps=10):\n",
    "    frames = images\n",
    "    imageio.mimwrite(f\"{save_dir}\", frames, fps=fps)\n",
    "\n",
    "\n",
    "def edit_image_principal_component(\n",
    "    g_ema,\n",
    "    mean_latent,\n",
    "    change_factor,\n",
    "    styles,\n",
    "    principal_component,\n",
    "    get_image=True,\n",
    "    plot=False,\n",
    "):\n",
    "    styles_copy = styles.clone().detach()\n",
    "    styles_copy = styles_copy + (principal_component.float() * change_factor)\n",
    "    image, _ = g_ema(\n",
    "    styles_copy, truncation=1, truncation_latent=mean_latent,input_is_latent=True,\n",
    "        )\n",
    "    image=image[0].permute(1,2,0).detach().numpy()\n",
    "    image_new = ((image - image.min()) * (1/(image.max() - image.min()) * 255)).astype('uint8')\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(image_new)\n",
    "        plt.show()\n",
    "    if get_image:\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Sampling a new image\n",
    "# sample_z = torch.randn(1,latent,device=device)\n",
    "# sample_w = g_ema.get_latent(sample_z).unsqueeze(0)\n",
    "# mean_latent = g_ema.mean_latent(4096)\n",
    "# torch.save(sample_w,\"./data/stylegan2_results/input_image.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing component 0\n",
      "Styles1\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [-0.9711652398109436, 0.8922786116600037]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [-0.9634853601455688, 0.8119815587997437]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "apply_to_all_layers=False\n",
    "latent_index=28\n",
    "class_index=13\n",
    "latent_name=\"car_shape\"\n",
    "component=9\n",
    "\n",
    "V= torch.load(\"data/comparison_data/principal_components_style_gan.pt\",map_location=torch.device('cpu')) \n",
    "\n",
    "for component in [0]:\n",
    "    print(f\"Processing component {component}\")\n",
    "    for i,style_chosen in enumerate([sample_w]):\n",
    "        print(f\"Styles{i+1}\")       \n",
    "        images=[]\n",
    "        segs=[]\n",
    "        for multiplier in  np.linspace(0,1,2):\n",
    "            #print(f\"Analyzing COMPONENT {component} with multiplier {multiplier}\")\n",
    "            image=edit_image_principal_component(g_ema,mean_latent,multiplier,sample_w,V[component],get_image=True,plot=False)\n",
    "            images.append(image)\n",
    "        # images = images + images[::-1]\n",
    "        # segs= segs + segs[::-1]\n",
    "    images_to_video(images,f\"./data/stylegan2_results/editing_principal_component_{component}.mp4\",fps=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8daaeadef7a295f68d0e95189507333c5c96e909e217a26b6b4a4818e648299"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
